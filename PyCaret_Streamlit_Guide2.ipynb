{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to PyCaret and Streamlit for Machine Learning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding PyCaret\n",
    "\n",
    "## What is PyCaret?\n",
    "\n",
    "PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, and many others.\n",
    "\n",
    "## Why Use PyCaret?\n",
    "\n",
    "Traditional machine learning workflows require extensive code for:\n",
    "- Data preprocessing (scaling, encoding, missing value imputation)\n",
    "- Feature engineering\n",
    "- Model training and evaluation\n",
    "- Hyperparameter tuning\n",
    "- Model deployment\n",
    "\n",
    "PyCaret reduces this complexity by providing a high-level API that handles these tasks automatically.\n",
    "\n",
    "## Traditional Approach vs PyCaret\n",
    "\n",
    "### Traditional scikit-learn approach (100+ lines of code):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "for col in X.select_dtypes(include='object').columns:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "```\n",
    "\n",
    "### PyCaret approach (5 lines of code):\n",
    "\n",
    "```python\n",
    "from pycaret.classification import *\n",
    "\n",
    "# Setup handles all preprocessing automatically\n",
    "clf = setup(data=data, target='target', session_id=42)\n",
    "\n",
    "# Compare all models automatically\n",
    "best_model = compare_models()\n",
    "```\n",
    "\n",
    "## PyCaret Modules\n",
    "\n",
    "PyCaret provides modules for different machine learning tasks:\n",
    "\n",
    "- **pycaret.classification** - Binary and multiclass classification\n",
    "- **pycaret.regression** - Regression tasks\n",
    "- **pycaret.clustering** - Unsupervised clustering\n",
    "- **pycaret.anomaly** - Anomaly detection\n",
    "- **pycaret.nlp** - Natural language processing\n",
    "- **pycaret.time_series** - Time series forecasting\n",
    "\n",
    "This notebook focuses on **classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: PyCaret Functions and Parameters\n",
    "\n",
    "We will examine each function in detail, including all parameters and their usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1: setup()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The `setup()` function initializes the training environment and creates the transformation pipeline. This is the most important function in PyCaret and must be called before any other functions.\n",
    "\n",
    "### What setup() Does\n",
    "\n",
    "1. Infers data types (numeric vs categorical)\n",
    "2. Splits data into train and test sets\n",
    "3. Handles missing values\n",
    "4. Encodes categorical variables\n",
    "5. Scales/normalizes numeric features\n",
    "6. Handles class imbalance (if specified)\n",
    "7. Removes outliers (if specified)\n",
    "8. Performs feature engineering (if specified)\n",
    "\n",
    "### Required Parameters\n",
    "\n",
    "```python\n",
    "clf = setup(\n",
    "    data=data,        # pandas DataFrame\n",
    "    target='column'   # name of target column\n",
    ")\n",
    "```\n",
    "\n",
    "### All Parameters with Explanations\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **data** | DataFrame | Required | Pandas DataFrame containing features and target |\n",
    "| **target** | str | Required | Name of the target column |\n",
    "| **train_size** | float | 0.7 | Proportion of data for training (0.7 = 70% train, 30% test) |\n",
    "| **test_data** | DataFrame | None | Separate DataFrame to use as test set |\n",
    "| **preprocess** | bool | True | Whether to apply preprocessing |\n",
    "| **imputation_type** | str | 'simple' | Method for handling missing values: 'simple' or 'iterative' |\n",
    "| **numeric_imputation** | str | 'mean' | Strategy for numeric missing values: 'mean', 'median', 'mode', 'knn' |\n",
    "| **categorical_imputation** | str | 'mode' | Strategy for categorical missing values: 'mode', 'constant' |\n",
    "| **categorical_features** | list | None | List of columns to treat as categorical |\n",
    "| **numeric_features** | list | None | List of columns to treat as numeric |\n",
    "| **date_features** | list | None | List of columns to treat as dates |\n",
    "| **text_features** | list | None | List of columns containing text |\n",
    "| **ignore_features** | list | None | List of columns to ignore during modeling |\n",
    "| **ordinal_features** | dict | None | Dictionary mapping ordinal features to their order |\n",
    "| **high_cardinality_features** | list | None | Categorical features with many unique values |\n",
    "| **handle_unknown_categorical** | bool | True | How to handle categories in deployment not seen during training |\n",
    "| **unknown_categorical_method** | str | 'least_frequent' | Method to handle unknown categories |\n",
    "| **normalize** | bool | False | Whether to normalize (scale) numeric features using Z-score |\n",
    "| **normalize_method** | str | 'zscore' | Normalization method: 'zscore', 'minmax', 'maxabs', 'robust' |\n",
    "| **transformation** | bool | False | Whether to apply power transformations to make data more Gaussian |\n",
    "| **transformation_method** | str | 'yeo-johnson' | Power transformation method: 'yeo-johnson' or 'quantile' |\n",
    "| **pca** | bool | False | Whether to apply Principal Component Analysis |\n",
    "| **pca_method** | str | 'linear' | PCA method: 'linear', 'kernel', 'incremental' |\n",
    "| **pca_components** | int/float | None | Number of components (int) or variance to retain (float between 0-1) |\n",
    "| **feature_selection** | bool | False | Whether to perform feature selection |\n",
    "| **feature_selection_method** | str | 'classic' | Method: 'univariate', 'classic', 'sequential' |\n",
    "| **feature_selection_estimator** | str | 'lightgbm' | Algorithm to use for feature importance |\n",
    "| **n_features_to_select** | int/float | 0.2 | Number of features to select |\n",
    "| **remove_outliers** | bool | False | Whether to remove outliers from training data |\n",
    "| **outliers_method** | str | 'iforest' | Outlier detection method: 'iforest', 'ee', 'lof' |\n",
    "| **outliers_threshold** | float | 0.05 | Percentage of outliers to remove |\n",
    "| **fix_imbalance** | bool | False | Whether to fix class imbalance in target variable |\n",
    "| **fix_imbalance_method** | object | None | Custom resampling object (default uses SMOTE) |\n",
    "| **remove_multicollinearity** | bool | False | Whether to remove highly correlated features |\n",
    "| **multicollinearity_threshold** | float | 0.9 | Correlation threshold above which to remove features |\n",
    "| **polynomial_features** | bool | False | Whether to create polynomial and interaction features |\n",
    "| **polynomial_degree** | int | 2 | Degree of polynomial features |\n",
    "| **bin_numeric_features** | list | None | List of numeric features to bin/discretize |\n",
    "| **group_features** | list | None | List of features to group (aggregate statistics) |\n",
    "| **session_id** | int | None | Random seed for reproducibility |\n",
    "| **log_experiment** | bool | False | Whether to log experiment to MLflow |\n",
    "| **experiment_name** | str | None | Name for MLflow experiment |\n",
    "| **log_plots** | bool | False | Whether to log plots to MLflow |\n",
    "| **log_profile** | bool | False | Whether to log data profile |\n",
    "| **log_data** | bool | False | Whether to log training and test data |\n",
    "| **verbose** | bool | True | Whether to print information during setup |\n",
    "\n",
    "### Common Usage Examples\n",
    "\n",
    "#### Basic setup:\n",
    "```python\n",
    "clf = setup(data=data, target='Churn', session_id=42)\n",
    "```\n",
    "\n",
    "#### Setup with normalization:\n",
    "```python\n",
    "clf = setup(\n",
    "    data=data,\n",
    "    target='Churn',\n",
    "    session_id=42,\n",
    "    normalize=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### Setup with imbalance handling:\n",
    "```python\n",
    "clf = setup(\n",
    "    data=data,\n",
    "    target='Churn',\n",
    "    session_id=42,\n",
    "    fix_imbalance=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### Full setup with multiple options:\n",
    "```python\n",
    "clf = setup(\n",
    "    data=data,\n",
    "    target='Churn',\n",
    "    session_id=42,\n",
    "    normalize=True,\n",
    "    categorical_features=['PhoneService', 'Contract'],\n",
    "    numeric_features=['tenure', 'MonthlyCharges'],\n",
    "    fix_imbalance=True,\n",
    "    remove_outliers=True,\n",
    "    remove_multicollinearity=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2: compare_models()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function trains and evaluates all available classification algorithms using cross-validation and returns the best model based on a specified metric.\n",
    "\n",
    "### Is This Function Required?\n",
    "\n",
    "**No.** This function is OPTIONAL. You can skip it and directly create a specific model using `create_model()`.\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- When you don't know which algorithm will perform best\n",
    "- When you want to quickly evaluate multiple algorithms\n",
    "- For initial exploratory analysis\n",
    "\n",
    "### When NOT to Use\n",
    "\n",
    "- When you already know which algorithm to use\n",
    "- When time is limited (compare_models is slower)\n",
    "- When you want full control over the algorithm\n",
    "\n",
    "### Available Algorithms\n",
    "\n",
    "When you run `compare_models()`, PyCaret tests these algorithms:\n",
    "\n",
    "| Algorithm ID | Full Name | Description |\n",
    "|--------------|-----------|-------------|\n",
    "| lr | Logistic Regression | Simple linear classifier |\n",
    "| knn | K-Nearest Neighbors | Instance-based learning |\n",
    "| nb | Naive Bayes | Probabilistic classifier |\n",
    "| dt | Decision Tree | Tree-based classifier |\n",
    "| svm | Support Vector Machine (Linear) | Linear SVM |\n",
    "| rbfsvm | SVM with RBF Kernel | Non-linear SVM |\n",
    "| gpc | Gaussian Process Classifier | Probabilistic non-parametric |\n",
    "| mlp | Multi-Layer Perceptron | Neural network |\n",
    "| ridge | Ridge Classifier | Regularized linear model |\n",
    "| rf | Random Forest | Ensemble of decision trees |\n",
    "| qda | Quadratic Discriminant Analysis | Non-linear discriminant |\n",
    "| ada | AdaBoost Classifier | Boosting algorithm |\n",
    "| gbc | Gradient Boosting Classifier | Gradient boosting |\n",
    "| lda | Linear Discriminant Analysis | Linear discriminant |\n",
    "| et | Extra Trees Classifier | Randomized decision trees |\n",
    "| xgboost | Extreme Gradient Boosting | Advanced gradient boosting |\n",
    "| lightgbm | Light Gradient Boosting | Fast gradient boosting |\n",
    "| catboost | CatBoost Classifier | Handles categorical data well |\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **include** | list | None | List of algorithm IDs to include |\n",
    "| **exclude** | list | None | List of algorithm IDs to exclude |\n",
    "| **fold** | int | 10 | Number of cross-validation folds |\n",
    "| **round** | int | 4 | Decimal places for metrics |\n",
    "| **cross_validation** | bool | True | Whether to use cross-validation |\n",
    "| **sort** | str | 'Accuracy' | Metric to sort by: 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1', 'Kappa', 'MCC' |\n",
    "| **n_select** | int | 1 | Number of top models to return |\n",
    "| **budget_time** | float | None | Maximum time in minutes for compare_models |\n",
    "| **turbo** | bool | True | When True, uses fewer iterations for faster comparison |\n",
    "| **errors** | str | 'ignore' | How to handle errors: 'ignore' or 'raise' |\n",
    "| **fit_kwargs** | dict | None | Custom parameters to pass to fit method |\n",
    "| **groups** | str | None | Column name for GroupKFold cross-validation |\n",
    "| **verbose** | bool | True | Whether to print progress |\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "#### Basic usage (returns best model):\n",
    "```python\n",
    "best_model = compare_models()\n",
    "```\n",
    "\n",
    "#### Sort by different metric:\n",
    "```python\n",
    "best_model = compare_models(sort='AUC')\n",
    "```\n",
    "\n",
    "#### Return top 3 models:\n",
    "```python\n",
    "top_3_models = compare_models(n_select=3)\n",
    "```\n",
    "\n",
    "#### Exclude specific models:\n",
    "```python\n",
    "best_model = compare_models(exclude=['knn', 'nb'])\n",
    "```\n",
    "\n",
    "#### Include only specific models:\n",
    "```python\n",
    "best_model = compare_models(include=['rf', 'xgboost', 'lightgbm'])\n",
    "```\n",
    "\n",
    "### How to Know Which Algorithm Was Selected\n",
    "\n",
    "```python\n",
    "best_model = compare_models()\n",
    "\n",
    "# Print the algorithm name\n",
    "print(f\"Selected Algorithm: {type(best_model).__name__}\")\n",
    "\n",
    "# Or print the full model object\n",
    "print(best_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3: create_model()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function trains a specific classification model. Use this when you know exactly which algorithm you want to use.\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- When you already know which algorithm to use\n",
    "- When you want faster training (trains only one model)\n",
    "- When your requirements specify a particular algorithm\n",
    "- When you want more control\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **estimator** | str or object | Required | Algorithm ID (e.g., 'rf', 'xgboost') or sklearn estimator object |\n",
    "| **fold** | int | 10 | Number of cross-validation folds |\n",
    "| **round** | int | 4 | Decimal places for metrics |\n",
    "| **cross_validation** | bool | True | Whether to use cross-validation |\n",
    "| **fit_kwargs** | dict | None | Custom parameters to pass to fit method |\n",
    "| **groups** | str | None | Column name for GroupKFold |\n",
    "| **verbose** | bool | True | Whether to print progress |\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "#### Create Random Forest:\n",
    "```python\n",
    "rf_model = create_model('rf')\n",
    "```\n",
    "\n",
    "#### Create XGBoost:\n",
    "```python\n",
    "xgb_model = create_model('xgboost')\n",
    "```\n",
    "\n",
    "#### Create Logistic Regression:\n",
    "```python\n",
    "lr_model = create_model('lr')\n",
    "```\n",
    "\n",
    "#### Create with fewer folds (faster):\n",
    "```python\n",
    "rf_model = create_model('rf', fold=5)\n",
    "```\n",
    "\n",
    "#### Create with custom sklearn estimator:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "custom_rf = RandomForestClassifier(n_estimators=200, max_depth=10)\n",
    "model = create_model(custom_rf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 4: tune_model()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function performs hyperparameter tuning on a trained model to improve its performance.\n",
    "\n",
    "### Is This Function Required?\n",
    "\n",
    "**No.** Tuning is OPTIONAL. You can skip this step and go directly to `finalize_model()`.\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- When you want to maximize model performance\n",
    "- When you have time for optimization\n",
    "- For production models where accuracy is critical\n",
    "\n",
    "### When NOT to Use\n",
    "\n",
    "- When time is limited\n",
    "- When the baseline model is good enough\n",
    "- For quick prototypes\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **estimator** | object | Required | Trained model object from create_model() or compare_models() |\n",
    "| **optimize** | str | 'Accuracy' | Metric to optimize: 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1', 'Kappa', 'MCC' |\n",
    "| **fold** | int | 10 | Number of cross-validation folds |\n",
    "| **round** | int | 4 | Decimal places for metrics |\n",
    "| **n_iter** | int | 10 | Number of iterations for random/grid search |\n",
    "| **custom_grid** | dict | None | Custom hyperparameter grid |\n",
    "| **search_library** | str | 'scikit-learn' | Library for hyperparameter search: 'scikit-learn', 'scikit-optimize', 'tune-sklearn', 'optuna' |\n",
    "| **search_algorithm** | str | 'random' | Search algorithm: 'random' or 'grid' |\n",
    "| **early_stopping** | bool/str | False | Whether to use early stopping |\n",
    "| **early_stopping_max_iters** | int | 10 | Maximum iterations without improvement |\n",
    "| **choose_better** | bool | True | Whether to return tuned model only if better than original |\n",
    "| **fit_kwargs** | dict | None | Custom parameters for fit method |\n",
    "| **groups** | str | None | Column name for GroupKFold |\n",
    "| **verbose** | bool | True | Whether to print progress |\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "#### Basic tuning:\n",
    "```python\n",
    "best_model = compare_models()\n",
    "tuned_model = tune_model(best_model)\n",
    "```\n",
    "\n",
    "#### Optimize for AUC:\n",
    "```python\n",
    "tuned_model = tune_model(best_model, optimize='AUC')\n",
    "```\n",
    "\n",
    "#### More iterations for better optimization:\n",
    "```python\n",
    "tuned_model = tune_model(best_model, n_iter=50)\n",
    "```\n",
    "\n",
    "#### Custom hyperparameter grid:\n",
    "```python\n",
    "custom_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "tuned_model = tune_model(rf_model, custom_grid=custom_params)\n",
    "```\n",
    "\n",
    "#### Using grid search instead of random:\n",
    "```python\n",
    "tuned_model = tune_model(best_model, search_algorithm='grid')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 5: finalize_model()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function trains the model on the complete dataset (training + test sets) to maximize learning before deployment.\n",
    "\n",
    "### Is This Function Required?\n",
    "\n",
    "**Yes.** This is REQUIRED before deployment. Always call this function before saving your model.\n",
    "\n",
    "### Why It's Important\n",
    "\n",
    "During training and tuning, models are trained only on the training set. The test set is reserved for evaluation. Before deployment, you want to train on ALL available data to get the best possible model.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **estimator** | object | Required | Trained model object |\n",
    "| **fit_kwargs** | dict | None | Custom parameters for fit method |\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "#### Finalize after compare_models:\n",
    "```python\n",
    "best_model = compare_models()\n",
    "final_model = finalize_model(best_model)\n",
    "```\n",
    "\n",
    "#### Finalize after tuning:\n",
    "```python\n",
    "best_model = compare_models()\n",
    "tuned_model = tune_model(best_model)\n",
    "final_model = finalize_model(tuned_model)\n",
    "```\n",
    "\n",
    "#### Finalize a specific model:\n",
    "```python\n",
    "rf_model = create_model('rf')\n",
    "final_model = finalize_model(rf_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 6: save_model()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function saves the trained model and the entire preprocessing pipeline to a file for later use in deployment.\n",
    "\n",
    "### Is This Function Required?\n",
    "\n",
    "**Yes, for deployment.** If you want to use the model later (in Streamlit, Flask, etc.), you must save it.\n",
    "\n",
    "### What Gets Saved\n",
    "\n",
    "- The trained model\n",
    "- All preprocessing transformations (scaling, encoding, etc.)\n",
    "- Feature names and types\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **model** | object | Required | Trained model object (preferably finalized) |\n",
    "| **model_name** | str | Required | Name for the saved model file (without extension) |\n",
    "| **model_only** | bool | False | If True, saves only model; if False, saves model + pipeline |\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "#### Basic save:\n",
    "```python\n",
    "save_model(final_model, 'my_model')\n",
    "# Creates: my_model.pkl\n",
    "```\n",
    "\n",
    "#### Save with path:\n",
    "```python\n",
    "save_model(final_model, 'models/churn_model')\n",
    "# Creates: models/churn_model.pkl\n",
    "```\n",
    "\n",
    "#### Save only model (no preprocessing):\n",
    "```python\n",
    "save_model(final_model, 'my_model', model_only=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 7: load_model()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function loads a previously saved model for making predictions.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **model_name** | str | Required | Name of saved model file (without .pkl extension) |\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "```python\n",
    "model = load_model('my_model')\n",
    "# or\n",
    "model = load_model('models/churn_model')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 8: predict_model()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function generates predictions on new data using a trained model.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **estimator** | object | Required | Trained model object |\n",
    "| **data** | DataFrame | None | Data to predict on (if None, uses test set) |\n",
    "| **round** | int | 4 | Decimal places for prediction probabilities |\n",
    "\n",
    "### Returns\n",
    "\n",
    "DataFrame with original data plus:\n",
    "- `prediction_label` - Predicted class\n",
    "- `prediction_score` - Prediction probability\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "#### Predict on test set:\n",
    "```python\n",
    "predictions = predict_model(final_model)\n",
    "```\n",
    "\n",
    "#### Predict on new data:\n",
    "```python\n",
    "new_data = pd.DataFrame({...})\n",
    "predictions = predict_model(final_model, data=new_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 9: plot_model()\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function creates visualizations for model evaluation and interpretation.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| **estimator** | object | Required | Trained model object |\n",
    "| **plot** | str | 'auc' | Type of plot to create |\n",
    "| **save** | bool | False | Whether to save plot as image |\n",
    "\n",
    "### Available Plots\n",
    "\n",
    "| Plot ID | Description |\n",
    "|---------|-------------|\n",
    "| auc | Area Under Curve (ROC) |\n",
    "| confusion_matrix | Confusion Matrix |\n",
    "| threshold | Discrimination Threshold |\n",
    "| pr | Precision-Recall Curve |\n",
    "| error | Class Prediction Error |\n",
    "| class_report | Classification Report |\n",
    "| boundary | Decision Boundary |\n",
    "| rfe | Recursive Feature Elimination |\n",
    "| learning | Learning Curve |\n",
    "| manifold | Manifold Learning |\n",
    "| calibration | Calibration Curve |\n",
    "| vc | Validation Curve |\n",
    "| dimension | Dimension Learning |\n",
    "| feature | Feature Importance |\n",
    "| feature_all | Feature Importance (All) |\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "```python\n",
    "# Plot ROC curve\n",
    "plot_model(final_model, plot='auc')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_model(final_model, plot='confusion_matrix')\n",
    "\n",
    "# Plot feature importance\n",
    "plot_model(final_model, plot='feature')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Complete Workflows\n",
    "\n",
    "Now that we understand all functions, let's see complete workflows with both required and optional steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning - REQUIRED BEFORE PYCARET\n",
    "\n",
    "You must clean your data before using PyCaret. This is NOT optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Workflow 1: Using compare_models (Automated Approach)\n",
    "\n",
    "This workflow lets PyCaret find the best algorithm automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup PyCaret Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tune the Model (OPTIONAL)\n",
    "\n",
    "You can skip this step if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finalize Model (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Model (REQUIRED for Deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Workflow 2: Using Specific Model (Manual Approach)\n",
    "\n",
    "This workflow uses a specific algorithm directly, skipping compare_models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup PyCaret Environment\n",
    "\n",
    "Setup is always required, no matter which workflow you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Specific Model\n",
    "\n",
    "Choose the algorithm you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tune the Model (OPTIONAL)\n",
    "\n",
    "Again, this is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finalize Model (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Model (REQUIRED for Deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Workflow 3: Minimal Approach (No Tuning, No Compare)\n",
    "\n",
    "This is the fastest workflow - just train and deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new customer data\n",
    "new_customer = pd.DataFrame({\n",
    "    'gender': ['Male'],\n",
    "    'SeniorCitizen': [0],\n",
    "    'Partner': ['Yes'],\n",
    "    'Dependents': ['No'],\n",
    "    'tenure': [24],\n",
    "    'PhoneService': ['Yes'],\n",
    "    'MultipleLines': ['Yes'],\n",
    "    'InternetService': ['Fiber optic'],\n",
    "    'OnlineSecurity': ['No'],\n",
    "    'Contract': ['Month-to-month'],\n",
    "    'PaperlessBilling': ['Yes'],\n",
    "    'PaymentMethod': ['Electronic check'],\n",
    "    'MonthlyCharges': [85.50],\n",
    "    'TotalCharges': [2052.00]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Understanding Streamlit\n",
    "\n",
    "## What is Streamlit?\n",
    "\n",
    "Streamlit is an open-source Python framework that allows you to create interactive web applications for machine learning and data science projects without needing to know HTML, CSS, or JavaScript.\n",
    "\n",
    "## Why Use Streamlit?\n",
    "\n",
    "Traditional web frameworks like Flask or Django require:\n",
    "- Frontend development (HTML, CSS, JavaScript)\n",
    "- Backend development (routing, form handling)\n",
    "- Template management\n",
    "- Complex deployment\n",
    "\n",
    "Streamlit simplifies this to pure Python code.\n",
    "\n",
    "## Streamlit vs Traditional Web Development\n",
    "\n",
    "### Traditional Flask Approach:\n",
    "\n",
    "```python\n",
    "# app.py (Backend)\n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.form.to_dict()\n",
    "    # More code...\n",
    "```\n",
    "\n",
    "Plus you need separate HTML files:\n",
    "\n",
    "```html\n",
    "<!-- index.html (Frontend) -->\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>My App</title>\n",
    "    <link rel=\"stylesheet\" href=\"style.css\">\n",
    "</head>\n",
    "<body>\n",
    "    <form method=\"POST\" action=\"/predict\">\n",
    "        <!-- Form fields -->\n",
    "    </form>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "### Streamlit Approach (Pure Python):\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"My ML App\")\n",
    "age = st.number_input(\"Enter age\")\n",
    "if st.button(\"Predict\"):\n",
    "    prediction = model.predict([[age]])\n",
    "    st.write(f\"Result: {prediction}\")\n",
    "```\n",
    "\n",
    "## Key Streamlit Features\n",
    "\n",
    "1. **Pure Python** - No HTML/CSS/JavaScript required\n",
    "2. **Interactive Widgets** - Buttons, sliders, text inputs, file uploaders\n",
    "3. **Data Display** - DataFrames, charts, images, maps\n",
    "4. **Fast Development** - Build apps in minutes, not hours\n",
    "5. **Easy Deployment** - Deploy to Streamlit Cloud for free\n",
    "6. **Automatic Rerun** - Page updates automatically when code changes\n",
    "\n",
    "## Common Streamlit Widgets\n",
    "\n",
    "```python\n",
    "# Text input\n",
    "name = st.text_input(\"Enter your name\")\n",
    "\n",
    "# Number input\n",
    "age = st.number_input(\"Enter age\", min_value=0, max_value=120)\n",
    "\n",
    "# Slider\n",
    "rating = st.slider(\"Rating\", 0, 10, 5)\n",
    "\n",
    "# Selectbox (dropdown)\n",
    "option = st.selectbox(\"Choose option\", [\"A\", \"B\", \"C\"])\n",
    "\n",
    "# Checkbox\n",
    "agree = st.checkbox(\"I agree\")\n",
    "\n",
    "# Button\n",
    "if st.button(\"Click me\"):\n",
    "    st.write(\"Button clicked!\")\n",
    "\n",
    "# File uploader\n",
    "file = st.file_uploader(\"Upload CSV\", type=['csv'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Streamlit App for Our Churn Model\n",
    "\n",
    "The following code creates a production-ready Streamlit app. Save this as `app.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the complete Streamlit app code\n",
    "# Save this as 'app.py' in the same folder as your saved model\n",
    "\n",
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from pycaret.classification import load_model, predict_model\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Customer Churn Prediction\",\n",
    "    page_icon=\"ðŸ“Š\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Load the saved model\n",
    "@st.cache_resource\n",
    "def load_churn_model():\n",
    "    return load_model('models/churn_model_rf')\n",
    "\n",
    "model = load_churn_model()\n",
    "\n",
    "# Title\n",
    "st.title(\"Customer Churn Prediction System\")\n",
    "st.markdown(\"\"\"\n",
    "This application predicts whether a customer will churn based on their profile.\n",
    "Enter the customer information in the sidebar and click 'Predict'.\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar for inputs\n",
    "st.sidebar.header(\"Customer Information\")\n",
    "\n",
    "# Demographics\n",
    "st.sidebar.subheader(\"Demographics\")\n",
    "gender = st.sidebar.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
    "senior_citizen = st.sidebar.selectbox(\"Senior Citizen\", [\"No\", \"Yes\"])\n",
    "partner = st.sidebar.selectbox(\"Has Partner\", [\"Yes\", \"No\"])\n",
    "dependents = st.sidebar.selectbox(\"Has Dependents\", [\"Yes\", \"No\"])\n",
    "\n",
    "# Account Information\n",
    "st.sidebar.subheader(\"Account Information\")\n",
    "tenure = st.sidebar.slider(\"Tenure (months)\", 1, 72, 24)\n",
    "contract = st.sidebar.selectbox(\n",
    "    \"Contract Type\", \n",
    "    [\"Month-to-month\", \"One year\", \"Two year\"]\n",
    ")\n",
    "paperless_billing = st.sidebar.selectbox(\"Paperless Billing\", [\"Yes\", \"No\"])\n",
    "payment_method = st.sidebar.selectbox(\n",
    "    \"Payment Method\",\n",
    "    [\"Electronic check\", \"Mailed check\", \"Bank transfer\", \"Credit card\"]\n",
    ")\n",
    "\n",
    "# Services\n",
    "st.sidebar.subheader(\"Services\")\n",
    "phone_service = st.sidebar.selectbox(\"Phone Service\", [\"Yes\", \"No\"])\n",
    "multiple_lines = st.sidebar.selectbox(\n",
    "    \"Multiple Lines\",\n",
    "    [\"Yes\", \"No\", \"No phone service\"]\n",
    ")\n",
    "internet_service = st.sidebar.selectbox(\n",
    "    \"Internet Service\",\n",
    "    [\"DSL\", \"Fiber optic\", \"No\"]\n",
    ")\n",
    "online_security = st.sidebar.selectbox(\n",
    "    \"Online Security\",\n",
    "    [\"Yes\", \"No\", \"No internet service\"]\n",
    ")\n",
    "\n",
    "# Billing\n",
    "st.sidebar.subheader(\"Billing\")\n",
    "monthly_charges = st.sidebar.number_input(\n",
    "    \"Monthly Charges\",\n",
    "    min_value=0.0,\n",
    "    max_value=200.0,\n",
    "    value=70.0,\n",
    "    step=5.0\n",
    ")\n",
    "total_charges = st.sidebar.number_input(\n",
    "    \"Total Charges\",\n",
    "    min_value=0.0,\n",
    "    max_value=10000.0,\n",
    "    value=1680.0,\n",
    "    step=100.0\n",
    ")\n",
    "\n",
    "# Predict button\n",
    "predict_button = st.sidebar.button(\"Predict Churn\", type=\"primary\")\n",
    "\n",
    "# Main area - split into two columns\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"Customer Profile\")\n",
    "    \n",
    "    # Create input dataframe\n",
    "    input_data = pd.DataFrame({\n",
    "        'gender': [gender],\n",
    "        'SeniorCitizen': [1 if senior_citizen == \"Yes\" else 0],\n",
    "        'Partner': [partner],\n",
    "        'Dependents': [dependents],\n",
    "        'tenure': [tenure],\n",
    "        'PhoneService': [phone_service],\n",
    "        'MultipleLines': [multiple_lines],\n",
    "        'InternetService': [internet_service],\n",
    "        'OnlineSecurity': [online_security],\n",
    "        'Contract': [contract],\n",
    "        'PaperlessBilling': [paperless_billing],\n",
    "        'PaymentMethod': [payment_method],\n",
    "        'MonthlyCharges': [monthly_charges],\n",
    "        'TotalCharges': [total_charges]\n",
    "    })\n",
    "    \n",
    "    # Display input as table\n",
    "    st.dataframe(input_data.T, use_container_width=True)\n",
    "\n",
    "with col2:\n",
    "    st.subheader(\"Prediction Results\")\n",
    "    \n",
    "    if predict_button:\n",
    "        with st.spinner(\"Making prediction...\"):\n",
    "            # Make prediction\n",
    "            prediction = predict_model(model, data=input_data)\n",
    "            \n",
    "            # Extract results\n",
    "            churn_prediction = prediction['prediction_label'][0]\n",
    "            churn_probability = prediction['prediction_score'][0]\n",
    "            \n",
    "            # Display results\n",
    "            if churn_prediction == \"Yes\":\n",
    "                st.error(\"HIGH RISK: Customer likely to churn\")\n",
    "                st.metric(\n",
    "                    \"Churn Probability\",\n",
    "                    f\"{churn_probability:.1%}\",\n",
    "                    delta=\"High Risk\",\n",
    "                    delta_color=\"inverse\"\n",
    "                )\n",
    "                st.warning(\"\"\"\n",
    "                **Recommended Actions:**\n",
    "                - Contact customer for retention offer\n",
    "                - Provide discount or upgrade incentive\n",
    "                - Improve customer service engagement\n",
    "                - Consider loyalty rewards\n",
    "                \"\"\")\n",
    "            else:\n",
    "                st.success(\"LOW RISK: Customer likely to stay\")\n",
    "                st.metric(\n",
    "                    \"Retention Probability\",\n",
    "                    f\"{(1 - churn_probability):.1%}\",\n",
    "                    delta=\"Low Risk\",\n",
    "                    delta_color=\"normal\"\n",
    "                )\n",
    "                st.info(\"\"\"\n",
    "                **Recommended Actions:**\n",
    "                - Maintain current service quality\n",
    "                - Explore upsell opportunities\n",
    "                - Request customer feedback\n",
    "                - Cross-sell additional services\n",
    "                \"\"\")\n",
    "    else:\n",
    "        st.info(\"Fill in customer details in the sidebar and click 'Predict Churn'\")\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"\"\"\n",
    "<div style='text-align: center'>\n",
    "    <p>Built with PyCaret and Streamlit</p>\n",
    "    <p>Blossom Academy - Data Science Program</p>\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "'''\n",
    "\n",
    "# Save the Streamlit app code\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"Streamlit app saved as 'app.py'\")\n",
    "print(\"\\nTo run the app:\")\n",
    "print(\"streamlit run app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run the Streamlit App\n",
    "\n",
    "### Step 1: Ensure you have the model saved\n",
    "\n",
    "Make sure you have run the model training code above and saved the model.\n",
    "\n",
    "### Step 2: Save the app.py file\n",
    "\n",
    "The cell above saves the Streamlit code as `app.py`.\n",
    "\n",
    "### Step 3: Run Streamlit\n",
    "\n",
    "Open your terminal or command prompt and navigate to the folder containing `app.py`, then run:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "### Step 4: Access the app\n",
    "\n",
    "Your browser will automatically open to `http://localhost:8501` showing your app.\n",
    "\n",
    "## Deploying to Streamlit Cloud (Free)\n",
    "\n",
    "1. Create a GitHub repository\n",
    "2. Upload your code:\n",
    "   - app.py\n",
    "   - requirements.txt (containing: pycaret, streamlit)\n",
    "   - Your saved model file\n",
    "3. Go to https://share.streamlit.io\n",
    "4. Sign in with GitHub\n",
    "5. Connect your repository\n",
    "6. Deploy\n",
    "7. Get a public URL like: https://your-app.streamlit.app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Required Steps\n",
    "\n",
    "1. Clean your data\n",
    "2. setup()\n",
    "3. Either compare_models() OR create_model()\n",
    "4. finalize_model()\n",
    "5. save_model()\n",
    "\n",
    "## Optional Steps\n",
    "\n",
    "1. tune_model() - Skip if you don't need hyperparameter optimization\n",
    "2. plot_model() - Skip if you don't need visualizations\n",
    "3. Parameters in setup() like fix_imbalance, remove_outliers - Use only if needed\n",
    "\n",
    "## Workflow Comparison\n",
    "\n",
    "| Workflow | Speed | Control | When to Use |\n",
    "|----------|-------|---------|-------------|\n",
    "| compare_models | Slow | Low | Exploration, don't know best algorithm |\n",
    "| create_model | Fast | High | Know which algorithm to use |\n",
    "| With tuning | Slow | Medium | Need best performance |\n",
    "| Without tuning | Fast | Medium | Good enough performance |\n",
    "\n",
    "## PyCaret + Streamlit Benefits\n",
    "\n",
    "1. Rapid prototyping - Build ML apps in hours, not days\n",
    "2. No frontend knowledge required - Pure Python\n",
    "3. Production-ready - Deploy to cloud for free\n",
    "4. Reproducible - session_id ensures consistent results\n",
    "5. Automated - Handles preprocessing automatically\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
